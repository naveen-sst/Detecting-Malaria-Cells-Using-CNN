{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification using Keras\n\nIn this notebook, I'll work with the Malaria Cell Images dataset to classify cells as either **Parasitized** or **Uninfected**. I'll use **Convolutional Neural Networks** to make the classification."},{"metadata":{},"cell_type":"markdown","source":"## Import libraries\n\nThe first step is to import all the necessary packages including `sklearn`, `pandas`, `numpy`, `matplotlib` and `keras`. I'll work with **Tensorflow** as the backend. I'll also import `Image`, `cv2` and `os` to work with images."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(1000)\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import dataset\n\nI'll now import the dataset and take a look at the two types of image data available. I'll use the image size to be 64x64."},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/cell_images/cell_images/'\nSIZE = 64\ndataset = []\nlabel = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parasitized cell images\n\nI iterate through all images in the **Parasitized** folder. I check if the file extension of the file being read is *png*.\nI then resize the image to 64x64 and then save it to the `dataset` variable as numpy array. The label for this is set as `0`."},{"metadata":{"trusted":true},"cell_type":"code","source":"parasitized_images = os.listdir(DATA_DIR + 'Parasitized/')\nfor i, image_name in enumerate(parasitized_images):\n    try:\n        if (image_name.split('.')[1] == 'png'):\n            image = cv2.imread(DATA_DIR + 'Parasitized/' + image_name)\n            image = Image.fromarray(image, 'RGB')\n            image = image.resize((SIZE, SIZE))\n            dataset.append(np.array(image))\n            label.append(0)\n    except Exception:\n        print(\"Could not read image {} with name {}\".format(i, image_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Uninfected cell images\n\nI iterate through all images in the **Uninfected** folder. I check if the file extension of the file being read is *png*.\nI then resize the image to 64x64 and then save it to the `dataset` variable as numpy array. The label for this is set as `0`."},{"metadata":{"trusted":true},"cell_type":"code","source":"uninfected_images = os.listdir(DATA_DIR + 'Uninfected/')\nfor i, image_name in enumerate(uninfected_images):\n    try:\n        if (image_name.split('.')[1] == 'png'):\n            image = cv2.imread(DATA_DIR + 'Uninfected/' + image_name)\n            image = Image.fromarray(image, 'RGB')\n            image = image.resize((SIZE, SIZE))\n            dataset.append(np.array(image))\n            label.append(1)\n    except Exception:\n        print(\"Could not read image {} with name {}\".format(i, image_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize data\n\nNow, I'll take a look at 5 random images from both **Parasitized** and **Uninfected** pools."},{"metadata":{},"cell_type":"markdown","source":"### Parasitized images\n\nI randomly select 10 values from the number of parasitized images and then display them in a row."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 12))\nfor index, image_index in enumerate(np.random.randint(len(parasitized_images), size = 10)):\n    plt.subplot(1, 10, index+1)\n    plt.imshow(dataset[image_index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Uninfected images\n\nI randomly select 10 values from the number of uninfected images and then display them in a row. I add the count of images of parasitized images to these index such that I am now showing images with label 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 12))\nfor index, image_index in enumerate(np.random.randint(len(uninfected_images), size = 10)):\n    plt.subplot(1, 10, index+1)\n    plt.imshow(dataset[len(parasitized_images) + image_index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying CNN\n\nI'll apply Convolutional Neural Networks with 2 Convolutional Layers followed by 2 Dense layers."},{"metadata":{},"cell_type":"markdown","source":"### Build the classifier\n\nI create a Sequential model with all the layers. I used the metric as `accuracy`."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = None\nclassifier = Sequential()\nclassifier.add(Convolution2D(32, (3, 3), input_shape = (SIZE, SIZE, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\nclassifier.add(BatchNormalization(axis = -1))\nclassifier.add(Dropout(0.2))\nclassifier.add(Convolution2D(32, (3, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\nclassifier.add(BatchNormalization(axis = -1))\nclassifier.add(Dropout(0.2))\nclassifier.add(Flatten())\nclassifier.add(Dense(activation = 'relu', units=512))\nclassifier.add(BatchNormalization(axis = -1))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(activation = 'relu', units=256))\nclassifier.add(BatchNormalization(axis = -1))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(activation = 'sigmoid', units=2))\nclassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nprint(classifier.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the dataset\n\nI split the dataset into training and testing dataset.\n1. Training data: 80%\n2. Testing data: 20%"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n\nX_train, X_test, y_train, y_test = train_test_split(dataset, to_categorical(np.array(label)), test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Keras, we can implement early stopping as a callback function. Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. Specifically, in our solution, we included EarlyStopping(monitor='val_loss', patience=2) to define that we wanted to monitor the test (validation) loss at each epoch and after the test loss has not improved after two epochs, training is interrupted. However, since we set patience=2, we wonâ€™t get the best model, but the model two epochs after the best model. Therefore, optionally, we can include a second operation, ModelCheckpoint which saves the model to a file after every checkpoint (which can be useful in case a multi-day training session is interrupted for some reason. Helpful for us, if we set save_best_only=True then ModelCheckpoint will only save the best model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set callback functions to early stop training and save the best model so far\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the model\n\nAs the training data is now ready, I will use it to train the classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = classifier.fit(np.array(X_train), \n                         y_train, \n                         batch_size = 64, \n                         verbose = 2, \n                         epochs = 50,\n                         callbacks = callbacks,\n                         validation_split = 0.1,\n                         shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy calculation\n\nI'll now calculate the accuracy on the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test_Accuracy: {:.2f}%\".format(classifier.evaluate(np.array(X_test), np.array(y_test))[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Improving the accuracy with Augmentation\n\nI will use **ImageDataGenerator** to generate more image data and then train the model on the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_generator = ImageDataGenerator(rescale = 1/255,\n                                     zoom_range = 0.3,\n                                     horizontal_flip = True,\n                                     rotation_range = 30)\n\ntest_generator = ImageDataGenerator(rescale = 1/255)\n\ntrain_generator = train_generator.flow(np.array(X_train),\n                                       y_train,\n                                       batch_size = 64,\n                                       shuffle = False)\n\ntest_generator = test_generator.flow(np.array(X_test),\n                                     y_test,\n                                     batch_size = 64,\n                                     shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I will use `fit_generator` methods to train the model and test on the validation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set callback functions to early stop training and save the best model so far\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2),\n             ModelCheckpoint(filepath='best_model_data_augmentatio.h5', monitor='val_loss', save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = classifier.fit_generator(train_generator,\n                                   steps_per_epoch = len(X_train)/64,\n                                   verbose =2,\n                                   epochs = 50,\n                                   callbacks = callbacks,\n                                   shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring new accuracy\n\nFinally, after training on augmented data, I'll check the accuracy on the testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test_Accuracy(after augmentation): {:.2f}%\".format(classifier.evaluate_generator(test_generator, steps = len(X_test), verbose = 1)[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, with **Data Augmentation** I was able to improve the accuracy further. Such a technique can be highly useful whenever we have limited dataset. This can ensure proper training of the model."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, I worked with the Malaria Cell Images dataset and applied **Convolutional Neural Networks** using Keras.\nI observed high accuracy which further increased with data augmentation."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}